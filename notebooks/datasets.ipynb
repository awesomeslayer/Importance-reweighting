{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63dc0dc-b3ce-4520-ad8d-7ea5419bc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import torch\n",
    "import statsmodels.api as sm\n",
    "from tqdm import trange, tqdm\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import gaussian_kde\n",
    "import time\n",
    "import colorama\n",
    "from colorama import Fore, Style\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import kstest\n",
    "from itertools import combinations\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, train_test_split\n",
    "from sklearn.utils import resample\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import dblquad, simps\n",
    "from scipy.spatial import KDTree\n",
    "from scipy import interpolate\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KernelDensity, NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score, log_loss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "\n",
    "solvers.options['disp'] = False\n",
    "solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba175a5d-3f99-436a-8c16-88b18529521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_mean_matching(g_train, g_test, kern='lin', B=1.0, eps=None):\n",
    "    nx = g_train.shape[0]\n",
    "    nz = g_test.shape[0]\n",
    "    \n",
    "    if eps is None:\n",
    "        eps = max(1e-6, B / np.sqrt(nz))  # Avoid very small values for uniform distributions\n",
    "    \n",
    "    if kern == 'lin':\n",
    "        K = np.dot(g_test, g_test.T)\n",
    "        kappa = np.sum(np.dot(g_test, g_train.T) * float(nz) / float(nx), axis=1)\n",
    "    elif kern == 'rbf':\n",
    "        K = compute_rbf(g_test, g_test, sigma=adjust_sigma(g_test))\n",
    "        kappa = np.sum(compute_rbf(g_test, g_train, sigma=adjust_sigma(g_test)), axis=1) * float(nz) / float(nx)\n",
    "    else:\n",
    "        raise ValueError('Unknown kernel')\n",
    "    \n",
    "    K = matrix(K)\n",
    "    kappa = matrix(kappa)\n",
    "    \n",
    "    # Regularization with dynamic epsilon\n",
    "    G = matrix(np.vstack([np.ones((1, nz)), -np.ones((1, nz)), np.eye(nz), -np.eye(nz)]))\n",
    "    h = matrix(np.hstack([nz * (1 + eps), nz * (eps - 1), B * np.ones(nz), np.zeros(nz)]))\n",
    "    \n",
    "    sol = solvers.qp(K, -kappa, G, h)\n",
    "    coef = np.array(sol['x']).flatten()\n",
    "    \n",
    "    # Clip the coefficients to avoid extreme values\n",
    "    coef = np.clip(coef, 0, B)\n",
    "    #print(f\"coef:{coef}\")\n",
    "    return coef\n",
    "\n",
    "def compute_rbf(X, Z, sigma=1.0):\n",
    "    \"\"\" Compute RBF kernel matrix \"\"\"\n",
    "    K = np.zeros((X.shape[0], Z.shape[0]), dtype=float)\n",
    "    for i, vx in enumerate(X):\n",
    "        K[i, :] = np.exp(-np.sum((vx - Z) ** 2, axis=1) / (2.0 * sigma))\n",
    "    return K\n",
    "\n",
    "def adjust_sigma(data):\n",
    "    \"\"\" Dynamically adjust sigma based on the variance of the data \"\"\"\n",
    "    pairwise_dists = np.sum((data[:, None] - data[None, :])**2, axis=-1)\n",
    "    median_dist = np.median(pairwise_dists)\n",
    "    return median_dist / np.log(len(data))  # Use median distance scaled by the log of the sample size\n",
    "\n",
    "def KMM_error(err, p_sample, g_sample, hyperparam):\n",
    "    coef = kernel_mean_matching(p_sample, g_sample, kern='rbf', B=hyperparam)\n",
    "    return logsumexp(err(g_sample) + np.log(coef)) - np.log(g_sample.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf9ea8-9a2f-4bb0-8153-939ca51b6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_lcf(pn_est_df, r=None, dim=None, est_name='LCF'):\n",
    "\n",
    "    if r is None:\n",
    "        r = pn_est_df['r'].values\n",
    "\n",
    "    non_zero_indices = np.where(pn_est_df['pn'] != 0)[0]\n",
    "    if len(non_zero_indices) == 0:\n",
    "        lcf_df = pd.DataFrame({'r': r, 'theo': 0, est_name: -1})\n",
    "        return lcf_df\n",
    "\n",
    "    first_non_zero_ind = non_zero_indices[0]\n",
    "\n",
    "    if np.any(np.isnan(pn_est_df['pn'])):\n",
    "        nan_indices = np.where(np.isnan(pn_est_df['pn']))[0]\n",
    "        if len(nan_indices) == 0:\n",
    "            last_ind = len(pn_est_df) - 1\n",
    "        else:\n",
    "            last_ind = min(len(pn_est_df) - 1, nan_indices[0] - 1)\n",
    "    else:\n",
    "        last_ind = len(pn_est_df) - 1\n",
    "\n",
    "    pn_est_defined = pn_est_df.iloc[first_non_zero_ind:last_ind + 1]\n",
    "\n",
    "    if dim is None:\n",
    "        dim = int(np.sqrt(len(pn_est_df)))\n",
    "    \n",
    "    dim = max(2, dim)  # Minimum 2 basis functions to ensure a valid spline degree\n",
    "    spline_degree = min(dim - 1, 5)  # Ensure degree is at most 5\n",
    "    \n",
    "    spline = interpolate.UnivariateSpline(pn_est_defined['r'], pn_est_defined['pn'], s=0, k=spline_degree, ext='raise')\n",
    "    spline_derivative = spline.derivative()\n",
    "\n",
    "    r_li = np.where(r >= pn_est_defined['r'].iloc[0])[0]\n",
    "    if len(r_li) > 0:\n",
    "        r_li = r_li[0]\n",
    "    else:\n",
    "        r_li = 0\n",
    "\n",
    "    r_hi = np.where(r > pn_est_defined['r'].iloc[-1])[0]\n",
    "    if len(r_hi) > 0:\n",
    "        r_hi = r_hi[0] - 1\n",
    "    else:\n",
    "        r_hi = len(r) - 1\n",
    "\n",
    "    if r_li <= r_hi:\n",
    "        r_def = r[r_li:r_hi+1]\n",
    "        pn = spline(r_def)\n",
    "        pn_deriv = spline_derivative(r_def)\n",
    "        pn_deriv = np.maximum(pn_deriv, 0)\n",
    "\n",
    "        lcf = np.where((pn > 0) & (pn_deriv >= 0), compute_lcf(r_def, pn, pn_deriv), -1)\n",
    "\n",
    "        num_ll_pad = r_li\n",
    "        num_na_pad = len(r) - r_hi - 1\n",
    "    else:\n",
    "        lcf = np.full(len(r), np.nan)\n",
    "        num_ll_pad = len(r)\n",
    "        num_na_pad = 0\n",
    "\n",
    "    lcf = np.concatenate([np.full(num_ll_pad, -1), lcf, np.full(num_na_pad, np.nan)])\n",
    "    lcf_df = pd.DataFrame({'r': r, 'theo': 0})\n",
    "    lcf_df[est_name] = lcf\n",
    "\n",
    "    return lcf_df\n",
    "\n",
    "def compute_lcf(r, pn, pn_deriv, lcf_lims=(-1, 1)):\n",
    "    if len(lcf_lims) != 2 or not all(isinstance(i, (int, float)) for i in lcf_lims):\n",
    "        raise ValueError(\"lcf_lims should be a tuple of two numeric values.\")\n",
    "\n",
    "    if lcf_lims[0] > lcf_lims[1]:\n",
    "        raise ValueError(\"The first limit of lcf_lims must be less than the second.\")\n",
    "\n",
    "    if not all(isinstance(arr, np.ndarray) for arr in [r, pn, pn_deriv]):\n",
    "        raise ValueError(\"r, pn, and pn_deriv must be numpy arrays.\")\n",
    "\n",
    "    if len(r) != len(pn) or len(pn) != len(pn_deriv):\n",
    "        raise ValueError(\"r, pn, and pn_deriv must be of the same length.\")\n",
    "\n",
    "    scale = lcf_lims[1] - lcf_lims[0]\n",
    "    shift = lcf_lims[0]\n",
    "    lcf = np.exp(-np.log(2) / 2 * r * pn_deriv / pn) * scale + shift\n",
    "    return lcf\n",
    "    \n",
    "def estimate_point_counts(random_points, r_values):\n",
    "    tree = KDTree(random_points)\n",
    "    num_points = len(random_points)\n",
    "\n",
    "    pn_list = []\n",
    "\n",
    "    for r in r_values:\n",
    "        counts = [len(tree.query_ball_point(point, r)) - 1 for point in random_points]\n",
    "        average_count = np.mean(counts)\n",
    "        pn_list.append([r, average_count])\n",
    "\n",
    "    pn_est_df = pd.DataFrame(pn_list, columns=['r', 'pn'])\n",
    "    \n",
    "    return pn_est_df\n",
    "\n",
    "\n",
    "def plot_lcf(x, ylim=(-1, 1), title=\"LCF Plot\", AUC = None):\n",
    "    plt.figure()\n",
    "    plt.plot(x['r'], x['LCF'], label=f'LCF with AUC = {AUC}', color='blue')\n",
    "    plt.axhline(y=0, color='grey', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('r')\n",
    "    plt.ylabel('LCF')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return True \n",
    "    \n",
    "def compute_normalized_auc(lcf_df, r_values, rmin=None, rmax=None):\n",
    "    if rmin is None:\n",
    "        rmin = np.min(r_values)\n",
    "    if rmax is None:\n",
    "        rmax = np.max(r_values)\n",
    "\n",
    "    mask = (r_values >= rmin) & (r_values <= rmax)\n",
    "    r_filtered = r_values[mask]\n",
    "    lcf_filtered = lcf_df['LCF'].values[mask]\n",
    "\n",
    "    delta_r = rmax - rmin\n",
    "\n",
    "    area_under_curve = np.trapz(lcf_filtered, r_filtered)\n",
    "\n",
    "    normalized_auc = area_under_curve/ delta_r\n",
    "\n",
    "    return normalized_auc\n",
    "\n",
    "def LCF_AUC_from_sample(sample, name, subname, r_values, rmin, rmax, flag = False):\n",
    "    est_df = estimate_point_counts(sample, r_values)\n",
    "    \n",
    "    lcf_df = estimate_lcf(est_df, r=r_values)\n",
    "\n",
    "    AUC = compute_normalized_auc(lcf_df, r_values, rmin, rmax)\n",
    "\n",
    "    if flag:\n",
    "        plot_lcf(\n",
    "                lcf_df,\n",
    "                title=f\"{name} -- {subname}\",\n",
    "                AUC=AUC,\n",
    "                )\n",
    "    return AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d4f01-a3d5-4437-ba04-40fb4011ba6e",
   "metadata": {},
   "source": [
    "# Cells dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f6789-8ff0-4fc9-b586-68a652b28594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = '..//datasets/cells'\n",
    "cells_list = [\"B cell\", \"Myeloid cell\"]\n",
    "use_pca = True  \n",
    "pca_num = 2      \n",
    "df_dict = {}\n",
    "DOWNSAMPLE_THRESHOLD = 2000 \n",
    "\n",
    "for cell_combination in combinations(cells_list, 2):\n",
    "    combo_key = \"-\".join(cell_combination)\n",
    "    df_dict.setdefault(combo_key, [])\n",
    "\n",
    "    for tma_folder in ['TMA9_1A_Scan1', 'TMA9_1B_Scan1', 'TMA9_2A_Scan1', 'TMA9_2B_Scan1']:\n",
    "        tma_path = os.path.join(base_dir, tma_folder)\n",
    "\n",
    "        for folder in os.listdir(tma_path):\n",
    "            if folder.startswith(\"T0\"):\n",
    "                folder_path = os.path.join(tma_path, folder)\n",
    "                subfolder_path = os.path.join(folder_path, \"1\")\n",
    "\n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    prediction_file = os.path.join(subfolder_path, 'prediction.txt')\n",
    "\n",
    "                    if os.path.isfile(prediction_file):\n",
    "                        try:\n",
    "                            df = pd.read_csv(prediction_file, header=None, sep='\\t')\n",
    "                            df.columns = ['Latitude', 'Longitude', 'Feature 1', 'Feature 2',\n",
    "                                          'Feature 3', 'Feature 4', 'Feature 5', 'Metrics']\n",
    "                        except pd.errors.EmptyDataError:\n",
    "                            print(f\"Warning: Empty or invalid file: {prediction_file}\")\n",
    "                            continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Error reading file {prediction_file}: {e}\")\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        df = df[df['Metrics'].isin(cell_combination)]\n",
    "                        label_mapping = {cell: idx for idx, cell in enumerate(cell_combination)}\n",
    "                        df['Metrics'] = df['Metrics'].map(label_mapping)\n",
    "\n",
    "                        df.dropna(subset=['Metrics'], inplace=True)\n",
    "                        df['Metrics'] = df['Metrics'].astype(int)\n",
    "\n",
    "\n",
    "                        if len(df['Metrics'].unique()) < 2 or len(df) < 1000: \n",
    "                            continue\n",
    "\n",
    "                        X = df.drop(columns=['Metrics'])\n",
    "                        y = df['Metrics']\n",
    "\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                            X, y, test_size=0.5, stratify=y, random_state=43\n",
    "                        )\n",
    "\n",
    "                        if len(X_train) > DOWNSAMPLE_THRESHOLD:\n",
    "                            X_train, y_train = resample(\n",
    "                                X_train, y_train,\n",
    "                                n_samples=DOWNSAMPLE_THRESHOLD,\n",
    "                                replace=False,  \n",
    "                                stratify=y_train,\n",
    "                                random_state=42\n",
    "                            )\n",
    "\n",
    "                        if len(X_test) > DOWNSAMPLE_THRESHOLD:\n",
    "                            X_test, y_test = resample(\n",
    "                                X_test, y_test,\n",
    "                                n_samples=DOWNSAMPLE_THRESHOLD,\n",
    "                                replace=False,  \n",
    "                                stratify=y_test,\n",
    "                                random_state=42\n",
    "                            )\n",
    "                        \n",
    "                        scaler = StandardScaler()\n",
    "\n",
    "                        if use_pca:\n",
    "                            if X_train.shape[0] < pca_num or X_train.shape[1] < pca_num:\n",
    "                                print(f\"Skipping {folder} for {combo_key} due to insufficient samples/features for PCA: {X_train.shape}\")\n",
    "                                continue\n",
    "                            pca = PCA(n_components=pca_num)\n",
    "                            X_train_transformed = pca.fit_transform(X_train)\n",
    "                            X_test_transformed = pca.transform(X_test)\n",
    "                            columns = [f'PC{i+1}' for i in range(pca_num)]\n",
    "                        else:\n",
    "                            X_train_transformed = X_train.values\n",
    "                            X_test_transformed = X_test.values\n",
    "                            columns = X_train.columns.tolist()\n",
    "\n",
    "                        X_train_scaled = scaler.fit_transform(X_train_transformed)\n",
    "                        X_test_scaled = scaler.transform(X_test_transformed)\n",
    "\n",
    "                        train_df = pd.DataFrame(X_train_scaled, columns=columns)\n",
    "                        train_df['Metrics'] = y_train.reset_index(drop=True)\n",
    "\n",
    "                        test_df = pd.DataFrame(X_test_scaled, columns=columns)\n",
    "                        test_df['Metrics'] = y_test.reset_index(drop=True)\n",
    "\n",
    "                        df_dict[combo_key].append((train_df, test_df, folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb623b-6c76-4025-bfe9-d4f6c0d6c31e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples_dict = {}\n",
    "\n",
    "for key in df_dict:\n",
    "    samples_dict[key] = {'auc_values': [], 'df_list': []}  \n",
    "    \n",
    "    for train_df, test_df, subname in df_dict[key]:\n",
    "        combined_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "        #edit radius limits for different dimensions for pattern detection\n",
    "        rmax = 0.188 \n",
    "        rmin = rmax / 10\n",
    "        r_values = np.arange(0, rmax, rmax / 50)\n",
    "        \n",
    "        sample = np.asarray(combined_df.drop(columns=['Metrics']).values.tolist())\n",
    "        \n",
    "        AUC = LCF_AUC_from_sample(sample, key, subname, r_values, rmin, rmax, flag=False)\n",
    "        samples_dict[key]['auc_values'].append(AUC)  \n",
    "        samples_dict[key]['df_list'].append((train_df, test_df, subname, AUC))  \n",
    "        \n",
    "        print(f\"AUC for {key}, {subname}: {AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb664f-f3c6-4e63-8c90-f6b66d1f0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_p_thr = 0.1\n",
    "AUC_g_thr = 0.15\n",
    "\n",
    "for key in samples_dict:\n",
    "    samples_dict[key]['df_list_p'] = []\n",
    "    samples_dict[key]['df_list_g'] = []\n",
    "    \n",
    "    for train_df, test_df, subname, AUC in samples_dict[key]['df_list']:\n",
    "        if np.abs(AUC) < np.abs(AUC_p_thr):\n",
    "            samples_dict[key]['df_list_p'].append((train_df, test_df, subname, AUC))\n",
    "        elif np.abs(AUC) > AUC_g_thr :\n",
    "            samples_dict[key]['df_list_g'].append((train_df, test_df, subname, AUC))\n",
    "\n",
    "for key in samples_dict:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))        \n",
    "    sns.boxplot(data=[samples_dict[key]['auc_values']], ax=ax)\n",
    "    ax.set_xticklabels(['AUC'])\n",
    "    ax.set_title(f\"Boxplot of AUC_p and AUC_g for {key}\")\n",
    "    ax.set_ylabel(\"AUC values\")\n",
    "    ax.axhline(AUC_p_thr, color='red', linestyle='--', label='AUC_p_thr')\n",
    "    ax.axhline(AUC_g_thr, color='blue', linestyle='--', label='AUC_g_thr')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for key in samples_dict:\n",
    "    print(f\"Combination: {key}\")\n",
    "    print(f\"Number of p-type samples: {len(samples_dict[key]['df_list_p'])}\")\n",
    "    print(f\"Number of g-type samples: {len(samples_dict[key]['df_list_g'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4e2ae-6ab1-4845-8413-44fd75878455",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_p_thr = 0.1\n",
    "AUC_g_thr = 0.15\n",
    "\n",
    "base_plot_dir = './pca'\n",
    "os.makedirs(base_plot_dir, exist_ok=True)\n",
    "\n",
    "for key in samples_dict: \n",
    "    samples_dict[key]['df_list_p'] = []\n",
    "    samples_dict[key]['df_list_g'] = []\n",
    "    \n",
    "    if 'df_list' in samples_dict[key] and samples_dict[key]['df_list']:\n",
    "        for train_df, test_df, subname, auc_score_val in samples_dict[key]['df_list']:\n",
    "            if not isinstance(auc_score_val, (int, float)):\n",
    "                print(f\"Warning: Invalid AUC value ({auc_score_val}) for {subname} in {key}. Skipping p/g classification.\")\n",
    "                continue\n",
    "\n",
    "            if np.abs(auc_score_val) < np.abs(AUC_p_thr):\n",
    "                samples_dict[key]['df_list_p'].append((train_df, test_df, subname, auc_score_val))\n",
    "            elif np.abs(auc_score_val) > AUC_g_thr:\n",
    "                samples_dict[key]['df_list_g'].append((train_df, test_df, subname, auc_score_val))\n",
    "    else:\n",
    "        print(f\"Info: No 'df_list' or empty 'df_list' for key {key} to classify into p/g types.\")\n",
    "\n",
    "# if use_pca and pca_num == 2:\n",
    "#     print(f\"\\nGenerating PCA plots and CSVs (use_pca={use_pca}, pca_num={pca_num})...\")\n",
    "#     for type_label, type_list_key in [('p', 'df_list_p'), ('g', 'df_list_g')]:\n",
    "#         type_specific_dir = os.path.join(base_plot_dir, type_label)\n",
    "#         os.makedirs(type_specific_dir, exist_ok=True)\n",
    "\n",
    "#         for combo_key_plot in samples_dict: # combo_key_plot is the cell combination string\n",
    "#             combo_specific_dir = os.path.join(type_specific_dir, combo_key_plot)\n",
    "#             os.makedirs(combo_specific_dir, exist_ok=True)\n",
    "\n",
    "#             if samples_dict[combo_key_plot] and type_list_key in samples_dict[combo_key_plot] and samples_dict[combo_key_plot][type_list_key]:\n",
    "#                 for train_df_plot, test_df_plot, subname_plot, auc_val_plot in samples_dict[combo_key_plot][type_list_key]:\n",
    "#                     if train_df_plot.empty and test_df_plot.empty:\n",
    "#                         continue\n",
    "                    \n",
    "#                     plot_df = pd.concat([train_df_plot, test_df_plot], ignore_index=True)\n",
    "\n",
    "#                     csv_filename = f\"{subname_plot}.csv\"\n",
    "#                     csv_path = os.path.join(combo_specific_dir, csv_filename)\n",
    "#                     try:\n",
    "#                         plot_df[['PC1', 'PC2', 'Metrics']].to_csv(csv_path, index=False)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error saving CSV {csv_path}: {e}\")\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     plt.figure(figsize=(10, 8))\n",
    "#                     scatter = plt.scatter(plot_df['PC1'], plot_df['PC2'], c=plot_df['Metrics'], cmap='coolwarm', alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "#                     plt.xlabel('Principal Component 1')\n",
    "#                     plt.ylabel('Principal Component 2')\n",
    "#                     plt.title(f'PCA ({type_label}-type) - {combo_key_plot} - {subname_plot}\\nAUC: {auc_val_plot:.3f}', fontsize=10)\n",
    "                    \n",
    "#                     handles, labels = scatter.legend_elements(prop=\"colors\", alpha=0.6)\n",
    "#                     legend_labels = [f'Class {label.split(\"{\")[-1].split(\"}\")[0]}' for label in labels]\n",
    "\n",
    "#                     plt.legend(handles, legend_labels, title=\"Classes\")\n",
    "#                     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                    \n",
    "#                     png_filename = f\"{subname_plot}.png\"\n",
    "#                     png_path = os.path.join(combo_specific_dir, png_filename)\n",
    "#                     try:\n",
    "#                         plt.savefig(png_path)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error saving PNG {png_path}: {e}\")\n",
    "#                     plt.close() \n",
    "# else:\n",
    "#     if not use_pca:\n",
    "#         print(\"\\nPCA is disabled (use_pca=False). Skipping PCA plot generation.\")\n",
    "#     elif pca_num != 2:\n",
    "#         print(f\"\\nPCA is enabled but pca_num is {pca_num} (not 2). Skipping 2D PCA plot generation.\")\n",
    "\n",
    "print(\"\\nGenerating AUC boxplots...\")\n",
    "for key in samples_dict: \n",
    "    if samples_dict[key] and 'auc_values' in samples_dict[key] and samples_dict[key]['auc_values']:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))        \n",
    "        sns.boxplot(y=samples_dict[key]['auc_values'], ax=ax, showfliers=True, color=\"lightblue\") # Using y for vertical boxplot\n",
    "        ax.set_ylabel(\"AUC values\")\n",
    "        ax.set_title(f\"Boxplot of AUC values for {key}\")\n",
    "        ax.axhline(AUC_p_thr, color='red', linestyle='--', label=f'AUC_p_thr ({AUC_p_thr:.2f})')\n",
    "        ax.axhline(AUC_g_thr, color='blue', linestyle='--', label=f'AUC_g_thr ({AUC_g_thr:.2f})')\n",
    "        ax.set_ylim(-0.05, 1.05) \n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Info: No 'auc_values' or empty list for key {key}. Skipping boxplot.\")\n",
    "\n",
    "print(\"\\nCounts of p-type and g-type samples:\")\n",
    "for key in samples_dict: \n",
    "    num_p_type = 0\n",
    "    if samples_dict[key] and 'df_list_p' in samples_dict[key]:\n",
    "        num_p_type = len(samples_dict[key]['df_list_p'])\n",
    "    \n",
    "    num_g_type = 0\n",
    "    if samples_dict[key] and 'df_list_g' in samples_dict[key]:\n",
    "        num_g_type = len(samples_dict[key]['df_list_g'])\n",
    "\n",
    "    total_samples = 0\n",
    "    if samples_dict[key] and 'df_list' in samples_dict[key]:\n",
    "        total_samples = len(samples_dict[key]['df_list'])\n",
    "\n",
    "    print(f\"Combination: {key}\")\n",
    "    print(f\"  Total processed samples: {total_samples}\")\n",
    "    print(f\"  Number of p-type samples (AUC < {AUC_p_thr:.2f}): {num_p_type}\")\n",
    "    print(f\"  Number of g-type samples (AUC > {AUC_g_thr:.2f}): {num_g_type}\")\n",
    "    print(f\"  Number of samples not classified as p or g: {total_samples - num_p_type - num_g_type}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3a984-7c57-4312-94ec-bc3e6d19351c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colorama.init(autoreset=True)\n",
    "\n",
    "def custom_log_loss(y_true, y_pred_prob):\n",
    "    epsilon = 1e-15  \n",
    "    y_pred_prob = np.clip(y_pred_prob, epsilon, 1 - epsilon)\n",
    "    log_loss_value = -np.mean(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
    "    \n",
    "    return log_loss_value\n",
    "\n",
    "def print_header(text, color=Fore.CYAN):\n",
    "    print(f\"\\n{color}{Style.BRIGHT}{'=' * 60}\")\n",
    "    print(f\"{color}{Style.BRIGHT} {text}\")\n",
    "    print(f\"{color}{Style.BRIGHT}{'=' * 60}{Style.RESET_ALL}\\n\")\n",
    "\n",
    "def print_subheader(text, color=Fore.YELLOW):\n",
    "    print(f\"\\n{color}{Style.BRIGHT} {text}\")\n",
    "    print(f\"{color}{Style.BRIGHT}{'-' * 40}{Style.RESET_ALL}\")\n",
    "\n",
    "def print_result(label, value, color=Fore.GREEN):\n",
    "    print(f\"{color}{label}: {Style.RESET_ALL}{value:.3f}\")\n",
    "\n",
    "def print_section_separator():\n",
    "    print(f\"{Fore.BLUE}{'-' * 60}{Style.RESET_ALL}\")\n",
    "\n",
    "def evaluate_model(model_name, model, X_train, y_train, X_test, y_test, p_features_train, p_features_test, df_p_test):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print_subheader(f\"Training {model_name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "    auc_test = roc_auc_score(y_test, y_pred_prob[:, 1])\n",
    "    print_result(\"ROC AUC\", auc_test)\n",
    "    \n",
    "    if auc_test <= 0.7:\n",
    "        print(f\"{Fore.RED}Too low ROC AUC, skipping this model{Style.RESET_ALL}\")\n",
    "        return None, None\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    kde_scott = gaussian_kde(X_train.T, bw_method='scott')\n",
    "    bw_scott = kde_scott.factor\n",
    "    \n",
    "    kde_scott_p = gaussian_kde(p_features_train.T, bw_method='scott')\n",
    "    bw_scott_p = kde_scott_p.factor\n",
    "\n",
    "    print_result(\"Bandwidth (source)\", bw_scott)\n",
    "    print_result(\"Bandwidth (target)\", bw_scott_p)\n",
    "    \n",
    "    coef = kernel_mean_matching(p_features_test, X_test, kern='rbf', B=1000)\n",
    "    \n",
    "    log_loss_list_g = []\n",
    "    log_loss_list_p = []\n",
    "    \n",
    "    for label, prob in zip(y_test, y_pred_prob):\n",
    "        log_loss_list_g.append(custom_log_loss(label, prob[1]))\n",
    "        \n",
    "    for label, prob in zip(df_p_test['Metrics'], model.predict_proba(df_p_test.drop(columns=['Metrics']))):\n",
    "        log_loss_list_p.append(custom_log_loss(label, prob[1]))\n",
    "    \n",
    "    mce_p = log_loss_list_p                        \n",
    "    mce_g = log_loss_list_g\n",
    "    \n",
    "    is_scott = [error * kde_scott_p(point_p) / kde_scott(point_g) \n",
    "                for error, point_g, point_p in zip(log_loss_list_g, X_test, p_features_test)]\n",
    "    \n",
    "    kmm = [error * weight for error, weight in zip(log_loss_list_g, coef)]\n",
    "    \n",
    "    X_combined = np.vstack([X_train, p_features_train])\n",
    "    y_combined = np.concatenate([np.ones(len(X_train)), np.zeros(len(p_features_train))])\n",
    "    \n",
    "    try:\n",
    "        domain_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "        domain_clf.fit(X_combined, y_combined)\n",
    "    \n",
    "        source_label_in_y_combined = 1\n",
    "        target_label_in_y_combined = 0\n",
    "    \n",
    "        idx_source_proba = -1\n",
    "        if hasattr(domain_clf, 'classes_'):\n",
    "            for i, class_val in enumerate(domain_clf.classes_):\n",
    "                if class_val == source_label_in_y_combined:\n",
    "                    idx_source_proba = i\n",
    "                    break\n",
    "            if idx_source_proba == -1:\n",
    "                 raise ValueError(f\"Source label {source_label_in_y_combined} not found in domain_clf.classes_ ({domain_clf.classes_}).\")\n",
    "        else: \n",
    "            raise AttributeError(\"domain_clf does not have 'classes_' attribute after fitting.\")\n",
    "    \n",
    "        domain_probs_p_source_given_x = domain_clf.predict_proba(X_test)[:, idx_source_proba]\n",
    "    \n",
    "        n_source_domain_train = np.sum(y_combined == source_label_in_y_combined)\n",
    "        n_target_domain_train = np.sum(y_combined == target_label_in_y_combined)\n",
    "    \n",
    "        if n_target_domain_train == 0:\n",
    "            print(f\"{Fore.YELLOW}Warning: n_target_domain_train is 0 for domain classifier. Weights might be ill-defined. Setting prior_ratio_correction to 1.0.{Style.RESET_ALL}\")\n",
    "            prior_ratio_correction = 1.0\n",
    "        elif n_source_domain_train == 0:\n",
    "            print(f\"{Fore.YELLOW}Warning: n_source_domain_train is 0 for domain classifier. Weights might be ill-defined (all zero). Setting prior_ratio_correction to 0.0.{Style.RESET_ALL}\")\n",
    "            prior_ratio_correction = 0.0 # Results in zero weights if target_likeness > 0\n",
    "        else:\n",
    "            prior_ratio_correction = n_source_domain_train / n_target_domain_train\n",
    "        \n",
    "        prob_target_likeness_given_x = 1.0 - domain_probs_p_source_given_x\n",
    "        \n",
    "        raw_density_ratios = prob_target_likeness_given_x / (domain_probs_p_source_given_x + 1e-10)\n",
    "        \n",
    "        classifier_weights = raw_density_ratios * prior_ratio_correction\n",
    "        classifier_weights = np.clip(classifier_weights, 1e-7, 1000)\n",
    "        \n",
    "        if len(log_loss_list_g) != len(classifier_weights):\n",
    "            raise ValueError(f\"Mismatch in length between log_loss_list_g ({len(log_loss_list_g)}) and classifier_weights ({len(classifier_weights)}). Ensure X_test corresponds to log_loss_list_g.\")\n",
    "    \n",
    "        classifier = [error * weight for error, weight in zip(log_loss_list_g, classifier_weights)]\n",
    "        classifier_mean = np.mean(classifier)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{Fore.RED}Classifier error: {e}{Style.RESET_ALL}\")\n",
    "        if 'log_loss_list_g' in locals() or 'log_loss_list_g' in globals():\n",
    "            try:\n",
    "                num_losses = len(log_loss_list_g)\n",
    "                classifier = [np.nan] * num_losses\n",
    "            except NameError:\n",
    "                classifier = [np.nan]\n",
    "                print(f\"{Fore.YELLOW}Warning: log_loss_list_g not defined when Classifier error occurred.{Style.RESET_ALL}\")\n",
    "        else:\n",
    "            classifier = [np.nan]\n",
    "        classifier_mean = np.nan\n",
    "    \n",
    "    mce_p_mean, mce_g_mean = np.mean(mce_p), np.mean(mce_g)\n",
    "    is_scott_mean, kmm_mean = np.mean(is_scott), np.mean(kmm)\n",
    "    \n",
    "    print_subheader(\"Mean Error Values\")\n",
    "    print_result(\"MCE_p (Target)\", mce_p_mean)\n",
    "    print_result(\"MCE_g (Source)\", mce_g_mean)\n",
    "    print_result(\"ISE\", is_scott_mean)\n",
    "    print_result(\"KMM\", kmm_mean)\n",
    "    print_result(\"Classifier\", classifier_mean)\n",
    "    \n",
    "    results = {\n",
    "        \"MCE_p\": mce_p_mean,\n",
    "        \"MCE_g\": mce_g_mean,\n",
    "        \"ISE\": is_scott_mean,\n",
    "        \"KMM\": kmm_mean,\n",
    "        \"Classifier\": classifier_mean\n",
    "    }\n",
    "    \n",
    "    return results, auc_test\n",
    "\n",
    "models = {\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=3, learning_rate=10, max_depth=1, subsample=0.1, max_features=0.1),\n",
    "    \n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        C=1,              \n",
    "        penalty='l1',           \n",
    "        max_iter=1,             \n",
    "        solver='liblinear',     \n",
    "    ),\n",
    "\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=1,         \n",
    "        max_depth=1,            \n",
    "        min_samples_leaf=1,     \n",
    "        min_samples_split=0.5,  \n",
    "        class_weight='balanced' \n",
    "    ),\n",
    "    \n",
    "    \"NeuralNetwork\": MLPClassifier(\n",
    "        hidden_layer_sizes=(8, 4),  \n",
    "        activation='relu',           \n",
    "        solver='adam',               \n",
    "        alpha=0.01,                 \n",
    "        batch_size='auto',           \n",
    "        learning_rate='constant',    \n",
    "        learning_rate_init=10,    \n",
    "        max_iter=5,                 \n",
    "        early_stopping=True,         \n",
    "        validation_fraction=0.1,     \n",
    "        n_iter_no_change=100        \n",
    "    )\n",
    "}\n",
    "\n",
    "all_results = {model_name: {'MCE_g': [], 'MCE_p': [], 'ISE': [], 'KMM': [], 'Classifier': [], 'ROC_AUC': []} \n",
    "               for model_name in models.keys()}\n",
    "\n",
    "for key in samples_dict:\n",
    "    print_header(f\"Dataset: {key}\")\n",
    "    \n",
    "    dataset_results = {model_name: {'MCE_g': [], 'MCE_p': [], 'ISE': [], 'KMM': [], 'Classifier': [], 'ROC_AUC': []} \n",
    "                    for model_name in models.keys()}\n",
    "    \n",
    "    for df_p_train, df_p_test, subname_p, AUC_p in samples_dict[key]['df_list_p']:\n",
    "        p_features_train = df_p_train.drop(columns=['Metrics']).values\n",
    "        p_features_test = df_p_test.drop(columns=['Metrics']).values\n",
    "        p_features = np.vstack((p_features_train, p_features_test))\n",
    "        \n",
    "        for df_g_train, df_g_test, subname_g, AUC_g in samples_dict[key]['df_list_g']: \n",
    "            print_subheader(f\"Source: {subname_g}, Target: {subname_p}\")\n",
    "            print(f\"Features - Source: {len(p_features)}, Target: {len(p_features)}\")\n",
    "            print_result(\"AUC (source)\", AUC_g)\n",
    "            print_result(\"AUC (target)\", AUC_p)\n",
    "            \n",
    "            X_train = df_g_train.drop(columns=['Metrics']).values\n",
    "            X_test = df_g_test.drop(columns=['Metrics']).values\n",
    "            y_train, y_test = df_g_train['Metrics'], df_g_test['Metrics']  \n",
    "            \n",
    "            for model_name, model in models.items():\n",
    "                print_subheader(f\"Model: {model_name}\")\n",
    "                \n",
    "                model_results, auc_score = evaluate_model(\n",
    "                    model_name, model, X_train, y_train, X_test, y_test, \n",
    "                    p_features_train, p_features_test, df_p_test\n",
    "                )\n",
    "                \n",
    "                if model_results:\n",
    "                    for method, value in model_results.items():\n",
    "                        dataset_results[model_name][method].append(value)\n",
    "                        all_results[model_name][method].append(value)\n",
    "                    \n",
    "                    dataset_results[model_name]['ROC_AUC'].append(auc_score)\n",
    "                    all_results[model_name]['ROC_AUC'].append(auc_score)\n",
    "                \n",
    "                print_section_separator()\n",
    "    \n",
    "    print_header(f\"Summary for dataset: {key}\")\n",
    "    \n",
    "    print_subheader(\"Average ROC AUC Scores\")\n",
    "    for model_name in models.keys():\n",
    "        roc_auc_values = dataset_results[model_name]['ROC_AUC']\n",
    "        if roc_auc_values:\n",
    "            avg_roc_auc = np.mean(roc_auc_values)\n",
    "            print_result(f\"{model_name}\", avg_roc_auc)\n",
    "    \n",
    "    print_section_separator()\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        print_subheader(f\"Model: {model_name}\")\n",
    "        mean_metrics_results = {'MCE_g': None, 'ISE': None, 'KMM': None, 'Classifier': None}\n",
    "        \n",
    "        for method, values in dataset_results[model_name].items():\n",
    "            if method != \"MCE_p\" and method != \"ROC_AUC\":\n",
    "                actual = np.array(dataset_results[model_name][\"MCE_p\"])\n",
    "                predicted = np.array(values)\n",
    "                \n",
    "                if len(actual) > 0:\n",
    "                    epsilon = 1e-10\n",
    "                    actual_adj = np.maximum(actual, epsilon)\n",
    "                    mape = np.mean(np.abs((actual - predicted) / actual_adj)) * 100\n",
    "                    rmse = np.sqrt(np.mean((actual - predicted) ** 2))\n",
    "                    rmspe = np.sqrt(np.mean(((actual - predicted) / actual_adj) ** 2)) * 100\n",
    "                    \n",
    "                    mean_metrics_results[method] = {\n",
    "                        'MAPE': mape,\n",
    "                        'RMSE': rmse,\n",
    "                        'RMSPE': rmspe\n",
    "                    }\n",
    "        \n",
    "        for metric_name in ['MAPE', 'RMSE', 'RMSPE']:\n",
    "            print_subheader(f\"{metric_name}\")\n",
    "            for method in ['MCE_g', 'ISE', 'KMM', 'Classifier']:\n",
    "                if mean_metrics_results[method]:\n",
    "                    print_result(f\"{method}\", mean_metrics_results[method][metric_name])\n",
    "    \n",
    "    print_section_separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a83d14-b684-4dac-b9db-14f5020ea522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_final_header(text, color=Fore.CYAN):\n",
    "    print(f\"\\n{color}{Style.BRIGHT}{'=' * 80}\")\n",
    "    print(f\"{color}{Style.BRIGHT}  {text}\")\n",
    "    print(f\"{color}{Style.BRIGHT}{'=' * 80}{Style.RESET_ALL}\\n\")\n",
    "\n",
    "def print_final_dataset_header(text, color=Fore.YELLOW):\n",
    "    print(f\"\\n{color}{Style.BRIGHT}Dataset: {text}{Style.RESET_ALL}\")\n",
    "\n",
    "def print_final_result(label, value, color=Fore.WHITE, precision=3):\n",
    "    if isinstance(value, (int, float, np.floating)):\n",
    "        print(f\"  {color}{label}: {value:.{precision}f}{Style.RESET_ALL}\")\n",
    "    else:\n",
    "        print(f\"  {color}{label}: {value}{Style.RESET_ALL}\")\n",
    "\n",
    "def custom_log_loss(y_true, y_pred_prob):\n",
    "    epsilon = 1e-15\n",
    "    y_pred_prob = np.clip(y_pred_prob, epsilon, 1 - epsilon)\n",
    "    log_losses = - (y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
    "    return log_losses\n",
    "\n",
    "def generate_models(n_models=2000):\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        if i < n_models // 2:\n",
    "            model = GradientBoostingClassifier(\n",
    "                random_state=i,\n",
    "                n_estimators=np.random.randint(5, 20),\n",
    "                max_depth=np.random.randint(1, 4),\n",
    "                learning_rate=np.random.uniform(0.01, 0.3),\n",
    "                subsample=np.random.uniform(0.3, 0.8)\n",
    "            )\n",
    "        else:\n",
    "            model = RandomForestClassifier(\n",
    "                random_state=i,\n",
    "                n_estimators=np.random.randint(5, 20),\n",
    "                max_depth=np.random.randint(1, 4),\n",
    "                max_features=np.random.uniform(0.3, 0.8),\n",
    "                min_samples_split=np.random.randint(5, 20)\n",
    "            )\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "K = 5\n",
    "n_models = 2000\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "if 'samples_dict_filtered' in locals() or 'samples_dict_filtered' in globals():\n",
    "    for key in samples_dict_filtered:\n",
    "\n",
    "        if len(samples_dict_filtered[key]['df_list_p']) == 0 or len(samples_dict_filtered[key]['df_list_g']) == 0:\n",
    "            continue\n",
    "\n",
    "        tuple_p = samples_dict_filtered[key]['df_list_p'][0]\n",
    "        tuple_g = samples_dict_filtered[key]['df_list_g'][0]\n",
    "\n",
    "        df_p_train, df_p_test, years_p, AUC_p_original = tuple_p[0],tuple_p[1],tuple_p[2],tuple_p[3]\n",
    "        df_g_train, df_g_test, years_g, AUC_g_original = tuple_g[0],tuple_g[1],tuple_g[2],tuple_g[3]\n",
    "\n",
    "        X_p_train = df_p_train.drop(columns=['Metrics'])\n",
    "        y_p_train = df_p_train['Metrics']\n",
    "        X_p_test = df_p_test.drop(columns=['Metrics'])\n",
    "        y_p_test = df_p_test['Metrics']\n",
    "\n",
    "        X_g_train = df_g_train.drop(columns=['Metrics'])\n",
    "        y_g_train = df_g_train['Metrics']\n",
    "        X_g_test = df_g_test.drop(columns=['Metrics'])\n",
    "        y_g_test = df_g_test['Metrics']\n",
    "\n",
    "        if X_g_train.shape[1] != X_p_train.shape[1] or X_g_test.shape[1] != X_p_test.shape[1] or X_g_train.shape[1] != X_g_test.shape[1]:\n",
    "             continue\n",
    "\n",
    "        n_features = X_g_train.shape[1]\n",
    "\n",
    "        if len(X_g_train) < 2 or len(X_p_train) < 2 or len(X_g_test) < 2 or len(X_p_test) < 2:\n",
    "             continue\n",
    "\n",
    "        models = generate_models(n_models)\n",
    "\n",
    "        model_info = []\n",
    "\n",
    "        kde_scott_g = None\n",
    "        try:\n",
    "            if len(X_g_train) >= 2 and X_g_train.shape[1] > 0:\n",
    "                 kde_scott_g = gaussian_kde(X_g_train.values.T, bw_method='scott')\n",
    "        except np.linalg.LinAlgError:\n",
    "             kde_scott_g = None\n",
    "        except ValueError:\n",
    "             kde_scott_g = None\n",
    "\n",
    "        kde_scott_p = None\n",
    "        try:\n",
    "            if len(X_p_train) >= 2 and X_p_train.shape[1] > 0:\n",
    "                 kde_scott_p = gaussian_kde(X_p_train.values.T, bw_method='scott')\n",
    "        except np.linalg.LinAlgError:\n",
    "             kde_scott_p = None\n",
    "        except ValueError:\n",
    "             kde_scott_p = None\n",
    "\n",
    "        X_combined_train = np.vstack([X_g_train.values, X_p_train.values])\n",
    "        y_combined_train = np.concatenate([np.ones(len(X_g_train)), np.zeros(len(X_p_train))])\n",
    "\n",
    "        domain_clf = None\n",
    "        classifier_weights_per_sample = None\n",
    "        idx_source_proba = -1\n",
    "\n",
    "        try:\n",
    "            if len(np.unique(y_combined_train)) > 1:\n",
    "                 domain_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "                 domain_clf.fit(X_combined_train, y_combined_train)\n",
    "\n",
    "                 source_label_in_y_combined = 1\n",
    "                 if hasattr(domain_clf, 'classes_'):\n",
    "                     class_list = list(domain_clf.classes_)\n",
    "                     if source_label_in_y_combined in class_list:\n",
    "                         idx_source_proba = class_list.index(source_label_in_y_combined)\n",
    "                     else:\n",
    "                         raise ValueError(f\"Source label {source_label_in_y_combined} not found in domain_clf.classes_ ({domain_clf.classes_}).\")\n",
    "                 else:\n",
    "                      raise AttributeError(\"domain_clf does not have 'classes_' attribute after fitting.\")\n",
    "\n",
    "                 n_source_domain_train = np.sum(y_combined_train == source_label_in_y_combined)\n",
    "                 n_target_domain_train = np.sum(y_combined_train == 0)\n",
    "\n",
    "                 if n_target_domain_train > 0 and n_source_domain_train > 0:\n",
    "                      prior_ratio_correction = n_source_domain_train / n_target_domain_train\n",
    "                 elif n_target_domain_train == 0:\n",
    "                      prior_ratio_correction = 1.0\n",
    "                 else:\n",
    "                      prior_ratio_correction = 0.0\n",
    "\n",
    "                 domain_probs_p_source_given_x = domain_clf.predict_proba(X_g_test.values)[:, idx_source_proba]\n",
    "                 prob_target_likeness_given_x = 1.0 - domain_probs_p_source_given_x\n",
    "                 raw_density_ratios = prob_target_likeness_given_x / (domain_probs_p_source_given_x + 1e-10)\n",
    "                 classifier_weights_per_sample = np.clip(raw_density_ratios * prior_ratio_correction, 1e-7, 1000)\n",
    "\n",
    "            else:\n",
    "                 domain_clf = None\n",
    "                 classifier_weights_per_sample = None\n",
    "\n",
    "        except Exception:\n",
    "            domain_clf = None\n",
    "            classifier_weights_per_sample = None\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "\n",
    "            try:\n",
    "                model.fit(X_g_train, y_g_train)\n",
    "\n",
    "                y_pred_prob_g_test = model.predict_proba(X_g_test)[:, 1] if len(X_g_test) > 0 else np.array([])\n",
    "                y_pred_prob_p_test = model.predict_proba(X_p_test)[:, 1] if len(X_p_test) > 0 else np.array([])\n",
    "\n",
    "                auc_g = np.nan\n",
    "                if len(np.unique(y_g_test)) > 1 and len(y_g_test) > 0:\n",
    "                     try:\n",
    "                         auc_g = roc_auc_score(y_g_test, y_pred_prob_g_test)\n",
    "                     except Exception:\n",
    "                         pass\n",
    "\n",
    "                auc_p = np.nan\n",
    "                if len(np.unique(y_p_test)) > 1 and len(y_p_test) > 0:\n",
    "                    try:\n",
    "                        auc_p = roc_auc_score(y_p_test, y_pred_prob_p_test)\n",
    "                    except Exception:\n",
    "                         pass\n",
    "\n",
    "                log_loss_list_g = custom_log_loss(y_g_test, y_pred_prob_g_test) if len(y_g_test) > 0 else np.array([])\n",
    "                log_loss_list_p = custom_log_loss(y_p_test, y_pred_prob_p_test) if len(y_p_test) > 0 else np.array([])\n",
    "\n",
    "                mce_p_mean = np.mean(log_loss_list_p) if len(log_loss_list_p) > 0 else np.nan\n",
    "                mce_g_mean = np.mean(log_loss_list_g) if len(log_loss_list_g) > 0 else np.nan\n",
    "\n",
    "                ise_mean = np.nan\n",
    "                if kde_scott_g is not None and kde_scott_p is not None and len(X_g_test) > 0:\n",
    "                    try:\n",
    "                         source_densities = kde_scott_g(X_g_test.values.T)\n",
    "                         target_densities = kde_scott_p(X_g_test.values.T)\n",
    "                         ise_weights = target_densities / (source_densities + 1e-10)\n",
    "                         ise_weights = np.clip(ise_weights, 1e-7, 1000)\n",
    "\n",
    "                         if len(log_loss_list_g) == len(ise_weights) and len(log_loss_list_g) > 0:\n",
    "                             ise_mean = np.mean(log_loss_list_g * ise_weights)\n",
    "                         else:\n",
    "                              ise_mean = np.nan\n",
    "\n",
    "                    except Exception:\n",
    "                        ise_mean = np.nan\n",
    "                elif len(X_g_test) == 0:\n",
    "                     pass\n",
    "\n",
    "                classifier_mean = np.nan\n",
    "                if classifier_weights_per_sample is not None and len(log_loss_list_g) == len(classifier_weights_per_sample) and len(log_loss_list_g) > 0:\n",
    "                    try:\n",
    "                        classifier_mean = np.mean(log_loss_list_g * classifier_weights_per_sample)\n",
    "                    except Exception:\n",
    "                        classifier_mean = np.nan\n",
    "                else:\n",
    "                     classifier_mean = np.nan\n",
    "\n",
    "                model_info.append({\n",
    "                    'model_id': i,\n",
    "                    'model_type': type(model).__name__,\n",
    "                    'auc_g': auc_g,\n",
    "                    'auc_p': auc_p,\n",
    "                    'mce_p': mce_p_mean,\n",
    "                    'mce_g': mce_g_mean,\n",
    "                    'ise': ise_mean,\n",
    "                    'classifier': classifier_mean,\n",
    "                    'log_loss_list_g': log_loss_list_g\n",
    "                })\n",
    "\n",
    "            except Exception:\n",
    "                model_info.append({\n",
    "                    'model_id': i,\n",
    "                    'model_type': type(model).__name__,\n",
    "                    'auc_g': np.nan,\n",
    "                    'auc_p': np.nan,\n",
    "                    'mce_p': np.nan,\n",
    "                    'mce_g': np.nan,\n",
    "                    'ise': np.nan,\n",
    "                    'classifier': np.nan,\n",
    "                    'log_loss_list_g': []\n",
    "                })\n",
    "\n",
    "        kmm_weights_per_sample = None\n",
    "\n",
    "        try:\n",
    "             if len(X_g_test) > 0 and len(X_p_test) > 0 and X_g_test.shape[1] == X_p_test.shape[1]:\n",
    "                kmm_weights_per_sample = kernel_mean_matching(X_g_test.values, X_p_test.values, kern='rbf', B=1000)\n",
    "\n",
    "                for info in model_info:\n",
    "                    kmm_estimate_for_model = np.nan\n",
    "                    if len(info['log_loss_list_g']) > 0 and len(info['log_loss_list_g']) == len(kmm_weights_per_sample):\n",
    "                         try:\n",
    "                             kmm_estimate_for_model = np.mean(info['log_loss_list_g'] * kmm_weights_per_sample)\n",
    "                         except Exception:\n",
    "                             pass\n",
    "\n",
    "                    info['kmm'] = kmm_estimate_for_model\n",
    "\n",
    "            else:\n",
    "                for info in model_info:\n",
    "                     info['kmm'] = np.nan\n",
    "\n",
    "        except Exception:\n",
    "            for info in model_info:\n",
    "                 info['kmm'] = np.nan\n",
    "\n",
    "        for info in model_info:\n",
    "             if 'log_loss_list_g' in info:\n",
    "                  del info['log_loss_list_g']\n",
    "\n",
    "        model_df = pd.DataFrame(model_info)\n",
    "\n",
    "        model_df = model_df.dropna(subset=['mce_p']).reset_index(drop=True)\n",
    "\n",
    "        if len(model_df) == 0:\n",
    "             final_results[key] = {'Note': f'No models with valid MCE_p found for analysis for {key}'}\n",
    "             continue\n",
    "\n",
    "        actual_k = min(K, len(model_df))\n",
    "\n",
    "        current_dataset_final_results = {}\n",
    "\n",
    "        oracle_top_k_indices = model_df['mce_p'].nsmallest(actual_k).index\n",
    "        oracle_selected_models = model_df.loc[oracle_top_k_indices]\n",
    "        r_final_oracle = oracle_selected_models['mce_p'].mean()\n",
    "        current_dataset_final_results['Oracle'] = r_final_oracle\n",
    "\n",
    "        estimation_methods = ['mce_g', 'ise', 'kmm', 'classifier']\n",
    "\n",
    "        for method in estimation_methods:\n",
    "\n",
    "            if method not in model_df.columns:\n",
    "                 current_dataset_final_results[method] = np.nan\n",
    "                 continue\n",
    "\n",
    "            estimated_risks_series = model_df[method].dropna()\n",
    "\n",
    "            if len(estimated_risks_series) == 0:\n",
    "                 current_dataset_final_results[method] = np.nan\n",
    "                 continue\n",
    "\n",
    "            top_k_indices_method = estimated_risks_series.nsmallest(actual_k).index\n",
    "            selected_models_method = model_df.loc[top_k_indices_method]\n",
    "\n",
    "            r_final_method = selected_models_method['mce_p'].mean()\n",
    "            current_dataset_final_results[method] = r_final_method\n",
    "\n",
    "        final_results[key] = current_dataset_final_results\n",
    "\n",
    "print_final_header(\"FINAL RESULTS SUMMARY (Mean R_final for Top-K Models)\")\n",
    "\n",
    "if not final_results:\n",
    "     print(f\"{Fore.RED}No datasets processed or no valid models found in any dataset.{Style.RESET_ALL}\")\n",
    "else:\n",
    "    for dataset in sorted(final_results.keys()):\n",
    "        results = final_results[dataset]\n",
    "        print_final_dataset_header(dataset)\n",
    "\n",
    "        if 'Note' in results:\n",
    "             print_final_result(\"Note\", results['Note'], color=Fore.RED)\n",
    "             continue\n",
    "\n",
    "        if 'Oracle' in results:\n",
    "             print_final_result(\"Oracle (True)\", results['Oracle'], color=Fore.GREEN)\n",
    "        else:\n",
    "             print_final_result(\"Oracle (True)\", \"N/A\", color=Fore.RED)\n",
    "\n",
    "        estimation_methods_to_print = ['mce_g', 'ise', 'kmm', 'classifier']\n",
    "        method_labels = {'mce_g': 'MCE_G (Empirical)', 'ise': 'ISE (KDE)', 'kmm': 'KMM (Placeholder)', 'classifier': 'Classifier (Domain)'}\n",
    "\n",
    "        for method_key in estimation_methods_to_print:\n",
    "            label = method_labels.get(method_key, method_key.upper())\n",
    "            if method_key in results:\n",
    "                value = results[method_key]\n",
    "                if np.isnan(value):\n",
    "                    print_final_result(label, \"NaN\", color=Fore.RED, precision=3)\n",
    "                else:\n",
    "                    print_final_result(label, value, color=Fore.WHITE)\n",
    "            else:\n",
    "                 print_final_result(label, \"N/A\", color=Fore.RED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305a9dc-2a91-40b1-9ece-6aeaa737670e",
   "metadata": {},
   "source": [
    "# Species dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a72225-db7f-449a-bd1b-e18ed012c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "     'anemone': pd.read_csv(\"../datasets/species_new/anemone.csv\"),\n",
    "    'caltha': pd.read_csv(\"../datasets/species_new/caltha.csv\"), \n",
    "   # 'hepatica': pd.read_csv(\"../datasets/species_new/hepatica.csv\"),\n",
    "  #  'convallaria': pd.read_csv(\"../datasets/species_new/convallaria.csv\"),\n",
    "    #'oxalis': pd.read_csv(\"../datasets/species_new/oxalis.csv\"),\n",
    " #  'prunus': pd.read_csv(\"../datasets/species_new/prunus.csv\"),\n",
    "    'tussilago': pd.read_csv(\"../datasets/species_new/tussilago.csv\"), \n",
    "  #  'vaccinium': pd.read_csv(\"../datasets/species_new/vaccinium.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a23d37-2cb3-4bda-8dc0-5d1ec8173b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "do_resampling = True  \n",
    "do_pca = True\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "samples_dict = {}\n",
    "\n",
    "for key in datasets_dict:\n",
    "    print(f\"\\n----- Processing {key} dataset -----\")\n",
    "    df = datasets_dict[key]\n",
    "    df = df.rename(columns={\n",
    "        'lat': 'decimalLongitude',\n",
    "        'long': 'decimalLatitude'\n",
    "    })\n",
    "    merged_df = df.dropna()\n",
    "    print(df[\"presence\"].unique())\n",
    "    df_list_p = []\n",
    "    df_list_g = []\n",
    "    start_year = 2010\n",
    "    end_year = 2024\n",
    "    \n",
    "    for i in range(start_year, end_year + 1):\n",
    "        set_g = list(range(start_year, i))\n",
    "        \n",
    "        set_p = list(range(i, end_year + 1))\n",
    "\n",
    "        years_g = set_g\n",
    "        years_p = set_p\n",
    "        for a in range(1):\n",
    "            for b in range(1):\n",
    "                for c in range(1):\n",
    "                    for d in range(1):\n",
    "                        if years_g and years_p:\n",
    "                            df_slice_g = merged_df[merged_df['year'].isin(years_g)]\n",
    "                            df_slice_p = merged_df[merged_df['year'].isin(years_p)]\n",
    "                            \n",
    "                            g_class_counts = df_slice_g['presence'].value_counts()\n",
    "                            p_class_counts = df_slice_p['presence'].value_counts()\n",
    "                            \n",
    "                            def balance_data(df_slice):\n",
    "                                X = df_slice.drop(['presence', 'year'], axis=1)\n",
    "                                y = df_slice['presence']\n",
    "                                \n",
    "                                if do_resampling and len(y.unique()) > 1 and min(y.value_counts()) < max(y.value_counts()) * 0.8:\n",
    "                                    smt = SMOTETomek(random_state=42)\n",
    "                                    X_res, y_res = smt.fit_resample(X, y)\n",
    "                                    balanced = pd.DataFrame(X_res, columns=X.columns)\n",
    "                                    balanced['presence'] = y_res\n",
    "                                    return balanced\n",
    "\n",
    "                                balanced = pd.DataFrame(X, columns=X.columns)\n",
    "                                balanced['presence'] = y\n",
    "                                return balanced\n",
    "                    \n",
    "                            balanced_g = balance_data(df_slice_g)\n",
    "                            balanced_p = balance_data(df_slice_p)\n",
    "                            \n",
    "                            target_size = max(len(balanced_g), len(balanced_p))\n",
    "                            balanced_g = balanced_g.sample(n=target_size, replace=True, random_state=42) if len(balanced_g) < target_size else balanced_g.sample(n=target_size, random_state=42)\n",
    "                            balanced_p = balanced_p.sample(n=target_size, replace=True, random_state=42) if len(balanced_p) < target_size else balanced_p.sample(n=target_size, random_state=42)\n",
    "                            \n",
    "                            balanced_g = balanced_g.rename(columns={\n",
    "                                'decimalLatitude': 'Latitude',\n",
    "                                'decimalLongitude': 'Longitude',\n",
    "                                'presence': 'Metrics'\n",
    "                            })\n",
    "                            balanced_p = balanced_p.rename(columns={\n",
    "                                'decimalLatitude': 'Latitude',\n",
    "                                'decimalLongitude': 'Longitude',\n",
    "                                'presence': 'Metrics'\n",
    "                            })\n",
    "                            \n",
    "                            X_source = balanced_g.drop(['Metrics'], axis=1)\n",
    "                            y_source = balanced_g['Metrics']\n",
    "                            X_target = balanced_p.drop(['Metrics'], axis=1)\n",
    "                            y_target = balanced_p['Metrics']\n",
    "                            \n",
    "                            X_source_train, X_source_test, y_source_train, y_source_test = train_test_split(X_source, y_source, test_size=0.3, random_state=42)\n",
    "                            \n",
    "                            X_target_train, X_target_test, y_target_train, y_target_test = train_test_split(X_target, y_target, test_size=0.3, random_state=42)\n",
    "                            \n",
    "                            if do_pca:  \n",
    "                                \n",
    "                                pca_source = PCA(n_components=n_components)  \n",
    "                                X_source_train_pca = pca_source.fit_transform(X_source_train)\n",
    "                                X_source_test_pca = pca_source.transform(X_source_test)\n",
    "                                \n",
    "                                pca_target = PCA(n_components=n_components) \n",
    "                                X_target_train_pca = pca_target.fit_transform(X_target_train)\n",
    "                                X_target_test_pca = pca_target.transform(X_target_test)\n",
    "                                \n",
    "                                \n",
    "                                scaler_source = StandardScaler()\n",
    "                                X_source_train_scaled = scaler_source.fit_transform(X_source_train_pca)\n",
    "                                X_source_test_scaled = scaler_source.transform(X_source_test_pca)\n",
    "                                \n",
    "                                scaler_target = StandardScaler()\n",
    "                                X_target_train_scaled = scaler_target.fit_transform(X_target_train_pca)\n",
    "                                X_target_test_scaled = scaler_target.transform(X_target_test_pca)\n",
    "                        \n",
    "                                \n",
    "                                train_final_source = pd.DataFrame(X_source_train_scaled, \n",
    "                                                                columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "                                test_final_source = pd.DataFrame(X_source_test_scaled, \n",
    "                                                               columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "                                \n",
    "                                train_final_target = pd.DataFrame(X_target_train_scaled, \n",
    "                                                                columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "                                test_final_target = pd.DataFrame(X_target_test_scaled, \n",
    "                                                               columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "                            else:  \n",
    "                                \n",
    "                                scaler_source = StandardScaler()\n",
    "                                X_source_train_scaled = scaler_source.fit_transform(X_source_train)\n",
    "                                X_source_test_scaled = scaler_source.transform(X_source_test)\n",
    "                                \n",
    "                                scaler_target = StandardScaler()\n",
    "                                X_target_train_scaled = scaler_target.fit_transform(X_target_train)\n",
    "                                X_target_test_scaled = scaler_target.transform(X_target_test)\n",
    "                                \n",
    "                                \n",
    "                                feature_names = X_source.columns.tolist()\n",
    "                                \n",
    "                                \n",
    "                                train_final_source = pd.DataFrame(X_source_train_scaled, columns=feature_names)\n",
    "                                test_final_source = pd.DataFrame(X_source_test_scaled, columns=feature_names)\n",
    "                                \n",
    "                                train_final_target = pd.DataFrame(X_target_train_scaled, columns=feature_names)\n",
    "                                test_final_target = pd.DataFrame(X_target_test_scaled, columns=feature_names)\n",
    "                        \n",
    "                            \n",
    "                            train_final_source['Metrics'] = y_source_train.values\n",
    "                            test_final_source['Metrics'] = y_source_test.values\n",
    "                            train_final_target['Metrics'] = y_target_train.values\n",
    "                            test_final_target['Metrics'] = y_target_test.values\n",
    "                            \n",
    "                            df_list_g.append((train_final_source, test_final_source, years_g))\n",
    "                            df_list_p.append((train_final_target, test_final_target, years_p))\n",
    "                            \n",
    "                        samples_dict[key] = {\n",
    "                            'df_list_p': df_list_p,\n",
    "                            'df_list_g': df_list_g,\n",
    "                        }\n",
    "for key in samples_dict:\n",
    "    print(f\"\\nDataset: {key}\")\n",
    "    for i, (train, test, _) in enumerate(samples_dict[key]['df_list_g']):\n",
    "        print(f\"Split {i+1}:\")\n",
    "        print(f\"Train shape: {train.shape}\")\n",
    "        print(f\"Test shape: {test.shape}\")\n",
    "        print(\"Train class distribution:\", Counter(train['Metrics']))\n",
    "        print(\"Test class distribution:\", Counter(test['Metrics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678ff38-6230-4ecb-be1f-c2277d684bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_samples_data = {}\n",
    "\n",
    "for key in samples_dict:\n",
    "    all_samples_data[key] = []\n",
    "    df_list_p = samples_dict[key]['df_list_p']\n",
    "    df_list_g = samples_dict[key]['df_list_g']\n",
    "    \n",
    "    auc_p_values = []\n",
    "    auc_g_values = []\n",
    "    \n",
    "    print(f\"Processing: {key}\")\n",
    "    for df_tuple_p, df_tuple_g in zip(df_list_p, df_list_g):\n",
    "        \n",
    "        df_p_train, df_p_test, years_p = df_tuple_p\n",
    "        df_g_train, df_g_test, years_g = df_tuple_g\n",
    "        \n",
    "        \n",
    "        df_p = pd.concat([df_p_train, df_p_test])\n",
    "        df_g = pd.concat([df_g_train, df_g_test])\n",
    "        \n",
    "        \n",
    "        percent_p = len(df_p[df_p['Metrics'] == 1])/len(df_p)*100\n",
    "        percent_g = len(df_g[df_g['Metrics'] == 1])/len(df_g)*100\n",
    "        \n",
    "        \n",
    "        sample_p = df_p.drop(columns=['Metrics']).values\n",
    "        sample_g = df_g.drop(columns=['Metrics']).values\n",
    "\n",
    "        #similarly edit radius based on dimension and task size, results are sensetive to it!\n",
    "        rmax = 0.188\n",
    "        rmin = rmax/10\n",
    "        r_values = np.arange(0, rmax, rmax/50)\n",
    "        \n",
    "        AUC_p = LCF_AUC_from_sample(sample_p, key, \"source\", r_values, rmin, rmax, False)\n",
    "        AUC_g = LCF_AUC_from_sample(sample_g, key, \"target\", r_values, rmin, rmax, False)\n",
    "        \n",
    "        all_samples_data[key].append({\n",
    "            'p_data': (df_p_train, df_p_test, years_p),\n",
    "            'g_data': (df_g_train, df_g_test, years_g),\n",
    "            'AUC_p': AUC_p,\n",
    "            'AUC_g': AUC_g,\n",
    "            'percent_p': percent_p,\n",
    "            'percent_g': percent_g\n",
    "        })\n",
    "        \n",
    "        auc_p_values.append(AUC_p)\n",
    "        auc_g_values.append(AUC_g)\n",
    "        \n",
    "        print(f\"AUC_p: {AUC_p:.3f}, AUC_g: {AUC_g:.3f}\")\n",
    "        print(f\"Class balance - P: {percent_p:.1f}%, G: {percent_g:.1f}%\\n\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=[auc_p_values, auc_g_values])\n",
    "    plt.xticks([0, 1], ['AUC_p', 'AUC_g'])\n",
    "    plt.title(f'AUC Distribution for {key}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf45d21-94bd-43d4-9601-00d596c23678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(AUC_p_thr=0.15, AUC_g_thr=0.15):\n",
    "    samples_dict_filtered = {}\n",
    "    \n",
    "    for key in all_samples_data:\n",
    "        samples_dict_filtered[key] = {'df_list_p': [], 'df_list_g': []}\n",
    "        print(f\"\\nFiltering: {key}\")\n",
    "        \n",
    "        for sample in all_samples_data[key]:\n",
    "            AUC_p = sample['AUC_p']\n",
    "            AUC_g = sample['AUC_g']\n",
    "            percent_p = sample['percent_p']\n",
    "            percent_g = sample['percent_g']\n",
    "            \n",
    "            print(f\"Checking sample - AUC_p: {AUC_p:.3f}, AUC_g: {AUC_g:.3f}\")\n",
    "            \n",
    "            if (AUC_p < AUC_p_thr and \n",
    "                abs(AUC_g) > AUC_g_thr and \n",
    "                percent_p < 100 and \n",
    "                percent_g < 100 and AUC_p > 0):\n",
    "                \n",
    "                print(\"--> Passed filter\")\n",
    "                samples_dict_filtered[key]['df_list_p'].append(\n",
    "                    (*sample['p_data'], \"source\", AUC_p)\n",
    "                )\n",
    "                samples_dict_filtered[key]['df_list_g'].append(\n",
    "                    (*sample['g_data'], \"target\", AUC_g)\n",
    "                )\n",
    "        \n",
    "        print(f\"{key}\")\n",
    "        print(f\"Retained {len(samples_dict_filtered[key]['df_list_p'])} p-samples\")\n",
    "        print(f\"Retained {len(samples_dict_filtered[key]['df_list_g'])} g-samples\")\n",
    "    \n",
    "    return samples_dict_filtered\n",
    "\n",
    "samples_dict_filtered = filter_samples(AUC_p_thr=0.15, AUC_g_thr=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fc945-7dc0-42ab-95f2-c616801c2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = './pca' \n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "def create_filename_from_years(years_meta_list, default_name=\"unknown_period\"):\n",
    "    if isinstance(years_meta_list, (list, tuple)) and len(years_meta_list) >= 1:\n",
    "        if len(years_meta_list) == 1:\n",
    "            return str(years_meta_list[0])\n",
    "        return f\"{years_meta_list[0]}-{years_meta_list[-1]}\"\n",
    "    return default_name\n",
    "\n",
    "for key in samples_dict_filtered:\n",
    "    print(f\"\\nProcessing cell combination: {key}\")\n",
    "\n",
    "    if 'df_list_p' in samples_dict_filtered[key] and samples_dict_filtered[key]['df_list_p']:\n",
    "        p_output_dir = os.path.join(base_output_dir, 'p', key)\n",
    "        os.makedirs(p_output_dir, exist_ok=True)\n",
    "        print(f\"  Saving P-type samples to: {p_output_dir}\")\n",
    "\n",
    "        for i, df_tuple_p in enumerate(samples_dict_filtered[key]['df_list_p']):\n",
    "            try:\n",
    "                df_p_train, df_p_test, years_p_meta, p_sample_label, auc_p = df_tuple_p\n",
    "            except ValueError:\n",
    "                print(f\"    Warning: Could not unpack P-type tuple #{i} for {key}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            df_p = pd.concat([df_p_train, df_p_test], ignore_index=True)\n",
    "\n",
    "            if 'Metrics' not in df_p.columns:\n",
    "                print(f\"    Warning: 'Metrics' column missing in P-sample data for {p_sample_label} ({key}). Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            df_p_features = df_p.drop(columns=['Metrics'], errors='ignore')\n",
    "\n",
    "            if df_p_features.shape[1] == 2:\n",
    "                feature_cols = df_p_features.columns.tolist()\n",
    "                filename_base = create_filename_from_years(years_p_meta, default_name=f\"p_sample_{i}\")\n",
    "                \n",
    "       \n",
    "                csv_path = os.path.join(p_output_dir, f\"{filename_base}.csv\")\n",
    "                try:\n",
    "                    df_p[feature_cols + ['Metrics']].to_csv(csv_path, index=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error saving CSV for P-sample {filename_base} ({key}): {e}\")\n",
    "                    continue\n",
    "\n",
    "              \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                try:\n",
    "                    scatter = plt.scatter(\n",
    "                        df_p[feature_cols[0]], df_p[feature_cols[1]],\n",
    "                        c=df_p['Metrics'], cmap='coolwarm', alpha=0.6, edgecolors='k', linewidth=0.5\n",
    "                    )\n",
    "                    plt.title(f\"P-type: {key} - {p_sample_label} ({filename_base})\\nAUC: {auc_p:.3f}\", fontsize=10)\n",
    "                    plt.xlabel(feature_cols[0])\n",
    "                    plt.ylabel(feature_cols[1])\n",
    "                    if len(df_p['Metrics'].unique()) > 1 : \n",
    "                         plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "                    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                    \n",
    "                    plot_path = os.path.join(p_output_dir, f\"{filename_base}.png\")\n",
    "                    plt.savefig(plot_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error plotting/saving PNG for P-sample {filename_base} ({key}): {e}\")\n",
    "                finally:\n",
    "                    plt.close() \n",
    "            else:\n",
    "                print(f\"    Skipping P-sample {p_sample_label} ({key}): Not 2 features (found {df_p_features.shape[1]}) for plotting.\")\n",
    "    else:\n",
    "        print(f\"  No P-type samples found for {key} or list is empty.\")\n",
    "\n",
    "    \n",
    "    if 'df_list_g' in samples_dict_filtered[key] and samples_dict_filtered[key]['df_list_g']:\n",
    "        g_output_dir = os.path.join(base_output_dir, 'g', key)\n",
    "        os.makedirs(g_output_dir, exist_ok=True)\n",
    "        print(f\"  Saving G-type samples to: {g_output_dir}\")\n",
    "\n",
    "        for i, df_tuple_g in enumerate(samples_dict_filtered[key]['df_list_g']):\n",
    "            try:\n",
    "    \n",
    "                df_g_train, df_g_test, years_g_meta, g_sample_label, auc_g = df_tuple_g\n",
    "            except ValueError:\n",
    "                print(f\"    Warning: Could not unpack G-type tuple #{i} for {key}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            df_g = pd.concat([df_g_train, df_g_test], ignore_index=True)\n",
    "\n",
    "            if 'Metrics' not in df_g.columns:\n",
    "                print(f\"    Warning: 'Metrics' column missing in G-sample data for {g_sample_label} ({key}). Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            df_g_features = df_g.drop(columns=['Metrics'], errors='ignore')\n",
    "\n",
    "            if df_g_features.shape[1] == 2:\n",
    "                feature_cols = df_g_features.columns.tolist()\n",
    "                filename_base = create_filename_from_years(years_g_meta, default_name=f\"g_sample_{i}\")\n",
    "\n",
    "               \n",
    "                csv_path = os.path.join(g_output_dir, f\"{filename_base}.csv\")\n",
    "                try:\n",
    "                    df_g[feature_cols + ['Metrics']].to_csv(csv_path, index=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error saving CSV for G-sample {filename_base} ({key}): {e}\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                try:\n",
    "                    scatter = plt.scatter(\n",
    "                        df_g[feature_cols[0]], df_g[feature_cols[1]],\n",
    "                        c=df_g['Metrics'], cmap='coolwarm', alpha=0.6, edgecolors='k', linewidth=0.5\n",
    "                    )\n",
    "                    plt.title(f\"G-type: {key} - {g_sample_label} ({filename_base})\\nAUC: {auc_g:.3f}\", fontsize=10)\n",
    "                    plt.xlabel(feature_cols[0])\n",
    "                    plt.ylabel(feature_cols[1])\n",
    "                    if len(df_g['Metrics'].unique()) > 1 : \n",
    "                        plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "                    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "                    plot_path = os.path.join(g_output_dir, f\"{filename_base}.png\")\n",
    "                    plt.savefig(plot_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error plotting/saving PNG for G-sample {filename_base} ({key}): {e}\")\n",
    "                finally:\n",
    "                    plt.close()\n",
    "            else:\n",
    "                print(f\"    Skipping G-sample {g_sample_label} ({key}): Not 2 features (found {df_g_features.shape[1]}) for plotting.\")\n",
    "    else:\n",
    "        print(f\"  No G-type samples found for {key} or list is empty.\")\n",
    "\n",
    "print(\"\\nFinished processing all samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bbc80-c09e-4ea1-979b-6a4309d38060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init(autoreset=True)\n",
    "\n",
    "def custom_log_loss(y_true, y_pred_prob):\n",
    "    epsilon = 1e-15  # Small constant to avoid log(0)\n",
    "    y_pred_prob = np.clip(y_pred_prob, epsilon, 1 - epsilon)\n",
    "    \n",
    "    log_loss_value = -np.mean(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
    "    \n",
    "    return log_loss_value\n",
    "\n",
    "def print_header(text):\n",
    "    \"\"\"Print a formatted header\"\"\"\n",
    "    print(f\"\\n{Fore.CYAN}{Style.BRIGHT}{'=' * 80}\")\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}  {text}\")\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}{'=' * 80}{Style.RESET_ALL}\")\n",
    "\n",
    "def print_subheader(text):\n",
    "    \"\"\"Print a formatted subheader\"\"\"\n",
    "    print(f\"\\n{Fore.GREEN}{Style.BRIGHT}{'-' * 60}\")\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT}  {text}\")\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT}{'-' * 60}{Style.RESET_ALL}\")\n",
    "\n",
    "def print_result(label, value, color=Fore.WHITE, precision=3):\n",
    "    \"\"\"Print a formatted result\"\"\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"{color}{label}: {value:.{precision}f}{Style.RESET_ALL}\")\n",
    "    else:\n",
    "        print(f\"{color}{label}: {value}{Style.RESET_ALL}\")\n",
    "\n",
    "models = {\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, penalty='l2', random_state=42, C=100,),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "        \"NeuralNetwork\": MLPClassifier(\n",
    "        hidden_layer_sizes=(8, 4),  \n",
    "        activation='relu',           \n",
    "        solver='adam',               \n",
    "        alpha=0.01,                 \n",
    "        batch_size='auto',           \n",
    "        learning_rate='constant',    \n",
    "        learning_rate_init=10,    \n",
    "        max_iter=5,                 \n",
    "        early_stopping=True,         \n",
    "        validation_fraction=0.1,     \n",
    "        n_iter_no_change=100        \n",
    "    )\n",
    "    \n",
    "}\n",
    "\n",
    "all_results = {model_name: {'MCE_g': [], 'MCE_p': [], 'ISE': [], 'KMM': [], 'Classifier': []} \n",
    "               for model_name in models.keys()}\n",
    "\n",
    "for key in samples_dict_filtered:\n",
    "    print_header(f\"Dataset: {key}\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print_subheader(f\"Model: {model_name}\")\n",
    "        results = {'MCE_g': [], 'MCE_p': [], 'ISE': [], 'KMM': [], 'Classifier': []}\n",
    "        \n",
    "        for tuple_p, tuple_g in zip(samples_dict_filtered[key]['df_list_p'], samples_dict_filtered[key]['df_list_g']):\n",
    "            if len(samples_dict_filtered[key]['df_list_p']) > 0: \n",
    "                df_p_train, df_p_test, AUC_p, years_p = tuple_p[0], tuple_p[1], tuple_p[3], tuple_p[2]  \n",
    "                df_g_train, df_g_test, AUC_g, years_g = tuple_g[0], tuple_g[1], tuple_g[3], tuple_g[2]\n",
    "                \n",
    " \n",
    "                X_p_train = df_p_train.drop(columns=['Metrics'])\n",
    "                y_p_train = df_p_train['Metrics']\n",
    "                X_p_test = df_p_test.drop(columns=['Metrics'])\n",
    "                y_p_test = df_p_test['Metrics']\n",
    "                \n",
    "                X_g_train = df_g_train.drop(columns=['Metrics'])\n",
    "                y_g_train = df_g_train['Metrics']\n",
    "                X_g_test = df_g_test.drop(columns=['Metrics'])\n",
    "                y_g_test = df_g_test['Metrics']\n",
    "                \n",
    "                clf = model\n",
    "                start_time = time.time()\n",
    "                clf.fit(X_g_train, y_g_train)\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                y_pred_prob_g = clf.predict_proba(X_g_test)[:, 1]\n",
    "                auc_test = roc_auc_score(y_g_test, y_pred_prob_g)\n",
    "                \n",
    "                print_result(\"Sample sizes\", f\"P train: {len(y_p_train)}, G train: {len(y_g_train)}\")\n",
    "                print_result(\"AUC values\", f\"P: {AUC_p}, G: {AUC_g}\")\n",
    "                print_result(\"Test ROC AUC\", auc_test, color=Fore.YELLOW)\n",
    "                print_result(\"Training time\", f\"{training_time:.2f} seconds\", color=Fore.CYAN)\n",
    "                \n",
    "                if auc_test > 0.7:\n",
    "                    log_loss_list_g = [custom_log_loss(y_true, y_pred) \n",
    "                                       for y_true, y_pred in zip(y_g_test, y_pred_prob_g)]\n",
    "                    \n",
    "                    y_pred_prob_p = clf.predict_proba(X_p_test)[:, 1]\n",
    "                    log_loss_list_p = [custom_log_loss(y_true, y_pred) \n",
    "                                       for y_true, y_pred in zip(y_p_test, y_pred_prob_p)]\n",
    "                    \n",
    "                    mce_p_mean = np.mean(log_loss_list_p)\n",
    "                    mce_g_mean = np.mean(log_loss_list_g)\n",
    "                    \n",
    "                    kde_scott_g = gaussian_kde(X_g_train.values.T, bw_method='scott')\n",
    "                    kde_scott_p = gaussian_kde(X_p_train.values.T, bw_method='scott')\n",
    "                    bw_scott_g = kde_scott_g.factor\n",
    "                    bw_scott_p = kde_scott_p.factor\n",
    "                    \n",
    "                    print_result(\"Bandwidth values\", f\"P: {bw_scott_p:.5f}, G: {bw_scott_g:.5f}\")\n",
    "                    \n",
    "                    is_scott = [error * kde_scott_p(point) / kde_scott_g(point) \n",
    "                                for error, point in zip(log_loss_list_g, X_g_test.values)]\n",
    "                    \n",
    "                    kmm = kernel_mean_matching(X_p_test.values, X_g_test.values, kern='lin', B=1000)\n",
    "                    kmm_mean = np.mean([error * w for error, w in zip(log_loss_list_g, kmm)])\n",
    "                    \n",
    "                    X_combined = np.concatenate([X_g_train.values, X_p_train.values])\n",
    "                    y_combined = np.concatenate([np.ones(len(X_g_train)), np.zeros(len(X_p_train))])\n",
    "                    \n",
    "                    domain_clf = LogisticRegression(max_iter=10, C = 100)\n",
    "                    domain_clf.fit(X_combined, y_combined)\n",
    "                    \n",
    "                    domain_probs = domain_clf.predict_proba(X_g_test.values)[:, 1]\n",
    "                    classifier_weights = np.clip((1) / (domain_probs + 1e-10), 1e-7, 2)\n",
    "                    classifier_mean = np.mean([error * weight for error, weight in zip(log_loss_list_g, classifier_weights)])\n",
    "                    \n",
    "                    print_result(\"MCE P\", mce_p_mean, color=Fore.MAGENTA)\n",
    "                    print_result(\"MCE G\", mce_g_mean, color=Fore.MAGENTA)\n",
    "                    print_result(\"ISE\", np.mean(is_scott), color=Fore.MAGENTA)\n",
    "                    print_result(\"KMM\", kmm_mean, color=Fore.MAGENTA)\n",
    "                    print_result(\"Classifier\", classifier_mean, color=Fore.MAGENTA)\n",
    "                    print_result(\"Years\", f\"{years_p} -> {years_g}\", color=Fore.WHITE)\n",
    "                    \n",
    "                    results[\"MCE_g\"].append(mce_g_mean)\n",
    "                    results[\"MCE_p\"].append(mce_p_mean) \n",
    "                    results[\"KMM\"].append(kmm_mean) \n",
    "                    results[\"ISE\"].append(np.mean(is_scott))\n",
    "                    results[\"Classifier\"].append(classifier_mean)\n",
    "                    \n",
    "                else:\n",
    "                    print_result(f\"Low ROC AUC for {key}\", auc_test, color=Fore.RED)\n",
    "                \n",
    "                print(f\"{Fore.WHITE}{'-' * 40}{Style.RESET_ALL}\")\n",
    "        \n",
    "        for method in results:\n",
    "            all_results[model_name][method].extend(results[method])\n",
    "        \n",
    "        if results[\"MCE_p\"]:  # Only if we have results\n",
    "            mean_metrics_results = {'MCE_g': None, 'ISE': None, 'KMM': None, 'Classifier': None}\n",
    "            print_subheader(f\"Metrics Summary for {model_name} on {key}\")\n",
    "            \n",
    "            for method, values_list in results.items():\n",
    "                if method != \"MCE_p\":\n",
    "                    actual = np.array(results[\"MCE_p\"])\n",
    "                    predicted = np.array(values_list)\n",
    "                    epsilon = 1e-10\n",
    "                    actual_adj = np.maximum(actual, epsilon)\n",
    "                    \n",
    "                    mape = np.mean(np.abs((actual - predicted) / actual_adj)) * 100\n",
    "                    rmse = np.sqrt(np.mean((actual - predicted) ** 2))\n",
    "                    rmspe = np.sqrt(np.mean(((actual - predicted) / actual_adj) ** 2)) * 100\n",
    "                    \n",
    "                    mean_metrics_results[method] = {\n",
    "                        'MAPE': mape,\n",
    "                        'RMSE': rmse,\n",
    "                        'RMSPE': rmspe\n",
    "                    }\n",
    "            \n",
    "            for metric_name in ['MAPE', 'RMSE', 'RMSPE']:\n",
    "                print(f\"\\n{Fore.BLUE}{Style.BRIGHT}{metric_name}:{Style.RESET_ALL}\")\n",
    "                for method in ['MCE_g', 'ISE', 'KMM', 'Classifier']:\n",
    "                    metric_value = mean_metrics_results[method][metric_name]\n",
    "                    print_result(f\"  {method}\", metric_value)\n",
    "\n",
    "print_header(\"OVERALL RESULTS\")\n",
    "\n",
    "for model_name in models:\n",
    "    model_results = all_results[model_name]\n",
    "    if model_results[\"MCE_p\"]:  # Only if we have results\n",
    "        mean_metrics_results_all = {'MCE_g': None, 'ISE': None, 'KMM': None, 'Classifier': None}\n",
    "        print_subheader(f\"Overall Metrics for {model_name}\")\n",
    "        \n",
    "        for method, values_list in model_results.items():\n",
    "            if method != \"MCE_p\":\n",
    "                actual = np.array(model_results[\"MCE_p\"])\n",
    "                predicted = np.array(values_list)\n",
    "                epsilon = 1e-10\n",
    "                actual_adj = np.maximum(actual, epsilon)\n",
    "                \n",
    "                mape = np.mean(np.abs((actual - predicted) / actual_adj)) * 100\n",
    "                rmse = np.sqrt(np.mean((actual - predicted) ** 2))\n",
    "                rmspe = np.sqrt(np.mean(((actual - predicted) / actual_adj) ** 2)) * 100\n",
    "                \n",
    "                mean_metrics_results_all[method] = {\n",
    "                    'MAPE': mape,\n",
    "                    'RMSE': rmse,\n",
    "                    'RMSPE': rmspe\n",
    "                }\n",
    "        \n",
    "        for metric_name in ['MAPE', 'RMSE', 'RMSPE']:\n",
    "            print(f\"\\n{Fore.BLUE}{Style.BRIGHT}{metric_name}:{Style.RESET_ALL}\")\n",
    "            for method in ['MCE_g', 'ISE', 'KMM', 'Classifier']:\n",
    "                metric_value = mean_metrics_results_all[method][metric_name]\n",
    "                print_result(f\"  {method}\", metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f09600-081e-47d9-be0f-3be52a30f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_log_loss(y_true, y_pred_prob):\n",
    "    epsilon = 1e-15\n",
    "    y_pred_prob = np.clip(y_pred_prob, epsilon, 1 - epsilon)\n",
    "    log_loss_value = -np.mean(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
    "    return log_loss_value\n",
    "\n",
    "def print_header(text):\n",
    "    print(f\"\\n{Fore.CYAN}{Style.BRIGHT}{'=' * 80}\")\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}  {text}\")\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}{'=' * 80}{Style.RESET_ALL}\")\n",
    "\n",
    "def print_result(label, value, color=Fore.WHITE, precision=3):\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"{color}{label}: {value:.{precision}f}{Style.RESET_ALL}\")\n",
    "    else:\n",
    "        print(f\"{color}{label}: {value}{Style.RESET_ALL}\")\n",
    "\n",
    "def generate_models(n_models=2000):\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        if i < n_models // 2:\n",
    "            model = GradientBoostingClassifier(\n",
    "                random_state=i,\n",
    "                n_estimators=np.random.randint(5, 20),\n",
    "                max_depth=np.random.randint(1, 4),\n",
    "                learning_rate=np.random.uniform(0.01, 0.3),\n",
    "                subsample=np.random.uniform(0.3, 0.8)\n",
    "            )\n",
    "        else:\n",
    "            model = RandomForestClassifier(\n",
    "                random_state=i,\n",
    "                n_estimators=np.random.randint(5, 20),\n",
    "                max_depth=np.random.randint(1, 4),\n",
    "                max_features=np.random.uniform(0.3, 0.8),\n",
    "                min_samples_split=np.random.randint(5, 20)\n",
    "            )\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "K = 5\n",
    "n_models = 2000\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "for key in samples_dict_filtered:\n",
    "    print_header(f\"Dataset: {key}\")\n",
    "    \n",
    "    if len(samples_dict_filtered[key]['df_list_p']) > 0:\n",
    "        tuple_p = samples_dict_filtered[key]['df_list_p'][0]\n",
    "        tuple_g = samples_dict_filtered[key]['df_list_g'][0]\n",
    "        \n",
    "        df_p_train, df_p_test, AUC_p, years_p = tuple_p[0], tuple_p[1], tuple_p[3], tuple_p[2]\n",
    "        df_g_train, df_g_test, AUC_g, years_g = tuple_g[0], tuple_g[1], tuple_g[3], tuple_g[2]\n",
    "        \n",
    "        X_p_train = df_p_train.drop(columns=['Metrics'])\n",
    "        y_p_train = df_p_train['Metrics']\n",
    "        X_p_test = df_p_test.drop(columns=['Metrics'])\n",
    "        y_p_test = df_p_test['Metrics']\n",
    "        \n",
    "        X_g_train = df_g_train.drop(columns=['Metrics'])\n",
    "        y_g_train = df_g_train['Metrics']\n",
    "        X_g_test = df_g_test.drop(columns=['Metrics'])\n",
    "        y_g_test = df_g_test['Metrics']\n",
    "        \n",
    "        models = generate_models(n_models)\n",
    "        \n",
    "        all_risks = {\n",
    "            'MCE_p': [],\n",
    "            'MCE_g': [],\n",
    "            'ISE': [],\n",
    "            'KMM': [],\n",
    "            'Classifier': []\n",
    "        }\n",
    "        \n",
    "        kde_scott_g = gaussian_kde(X_g_train.values.T, bw_method='scott')\n",
    "        kde_scott_p = gaussian_kde(X_p_train.values.T, bw_method='scott')\n",
    "        \n",
    "        X_combined = np.concatenate([X_g_train.values, X_p_train.values])\n",
    "        y_combined = np.concatenate([np.ones(len(X_g_train)), np.zeros(len(X_p_train))])\n",
    "        domain_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "        domain_clf.fit(X_combined, y_combined)\n",
    "        \n",
    "        model_info = []\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processing model {i}/{n_models}\")\n",
    "            \n",
    "            model.fit(X_g_train, y_g_train)\n",
    "            \n",
    "            y_pred_prob_g = model.predict_proba(X_g_test)[:, 1]\n",
    "            y_pred_prob_p = model.predict_proba(X_p_test)[:, 1]\n",
    "            \n",
    "            auc_g = roc_auc_score(y_g_test, y_pred_prob_g)\n",
    "            auc_p = roc_auc_score(y_p_test, y_pred_prob_p)\n",
    "            \n",
    "            log_loss_list_g = [custom_log_loss(y_true, y_pred) \n",
    "                              for y_true, y_pred in zip(y_g_test, y_pred_prob_g)]\n",
    "            log_loss_list_p = [custom_log_loss(y_true, y_pred) \n",
    "                              for y_true, y_pred in zip(y_p_test, y_pred_prob_p)]\n",
    "            \n",
    "            mce_p_mean = np.mean(log_loss_list_p)\n",
    "            mce_g_mean = np.mean(log_loss_list_g)\n",
    "            \n",
    "            is_scott = [error * kde_scott_p(point) / kde_scott_g(point) \n",
    "                       for error, point in zip(log_loss_list_g, X_g_test.values)]\n",
    "            \n",
    "            kmm = kernel_mean_matching(X_p_test.values, X_g_test.values, kern='rbf', B=1000)\n",
    "            # print(kmm)\n",
    "            kmm_mean = np.mean([error * w for error, w in zip(log_loss_list_g, kmm)])\n",
    "            \n",
    "            domain_probs = domain_clf.predict_proba(X_g_test.values)[:, 1]\n",
    "            classifier_weights = np.clip((1) / (domain_probs), 1e-15, 2000)\n",
    "            # print(classifier_weights)\n",
    "            classifier_mean = np.mean([error * weight for error, weight in zip(log_loss_list_g, classifier_weights)])\n",
    "            \n",
    "            all_risks['MCE_p'].append(mce_p_mean)\n",
    "            all_risks['MCE_g'].append(mce_g_mean)\n",
    "            all_risks['ISE'].append(np.mean(is_scott))\n",
    "            all_risks['KMM'].append(kmm_mean)\n",
    "            all_risks['Classifier'].append(classifier_mean)\n",
    "            \n",
    "            model_info.append({\n",
    "                'model_id': i,\n",
    "                'model_type': type(model).__name__,\n",
    "                'auc_g': auc_g,\n",
    "                'auc_p': auc_p,\n",
    "                'mce_p': mce_p_mean,\n",
    "                'mce_g': mce_g_mean,\n",
    "                'ise': np.mean(is_scott),\n",
    "                'kmm': kmm_mean,\n",
    "                'classifier': classifier_mean,\n",
    "                'kde_p_factor': kde_scott_p.factor,\n",
    "                'kde_g_factor': kde_scott_g.factor,\n",
    "                'domain_prob_mean': np.mean(domain_probs),\n",
    "                'domain_prob_std': np.std(domain_probs),\n",
    "                'classifier_weights_mean': np.mean(classifier_weights),\n",
    "                'classifier_weights_std': np.std(classifier_weights),\n",
    "                'kmm_weights_mean': np.mean(kmm),\n",
    "                'kmm_weights_std': np.std(kmm),\n",
    "                'ise_weights_mean': np.mean([kde_scott_p(point) / kde_scott_g(point) for point in X_g_test.values]),\n",
    "                'ise_weights_std': np.std([kde_scott_p(point) / kde_scott_g(point) for point in X_g_test.values])\n",
    "            })\n",
    "        \n",
    "        print_header(f\"Model Statistics for {key}\")\n",
    "        model_df = pd.DataFrame(model_info)\n",
    "        \n",
    "        print_result(\"AUC G - Mean\", model_df['auc_g'].mean(), color=Fore.BLUE)\n",
    "        print_result(\"AUC G - Std\", model_df['auc_g'].std(), color=Fore.BLUE)\n",
    "        print_result(\"AUC G - Min\", model_df['auc_g'].min(), color=Fore.BLUE)\n",
    "        print_result(\"AUC G - Max\", model_df['auc_g'].max(), color=Fore.BLUE)\n",
    "        \n",
    "        print_result(\"AUC P - Mean\", model_df['auc_p'].mean(), color=Fore.BLUE)\n",
    "        print_result(\"AUC P - Std\", model_df['auc_p'].std(), color=Fore.BLUE)\n",
    "        print_result(\"AUC P - Min\", model_df['auc_p'].min(), color=Fore.BLUE)\n",
    "        print_result(\"AUC P - Max\", model_df['auc_p'].max(), color=Fore.BLUE)\n",
    "        \n",
    "        print_result(\"Domain Prob Mean\", model_df['domain_prob_mean'].mean(), color=Fore.CYAN)\n",
    "        print_result(\"Domain Prob Std\", model_df['domain_prob_std'].mean(), color=Fore.CYAN)\n",
    "        \n",
    "        print_result(\"Classifier Weights Mean\", model_df['classifier_weights_mean'].mean(), color=Fore.CYAN)\n",
    "        print_result(\"Classifier Weights Std\", model_df['classifier_weights_std'].mean(), color=Fore.CYAN)\n",
    "        \n",
    "        print_result(\"KMM Weights Mean\", model_df['kmm_weights_mean'].mean(), color=Fore.CYAN)\n",
    "        print_result(\"KMM Weights Std\", model_df['kmm_weights_std'].mean(), color=Fore.CYAN)\n",
    "        \n",
    "        print_result(\"ISE Weights Mean\", model_df['ise_weights_mean'].mean(), color=Fore.CYAN)\n",
    "        print_result(\"ISE Weights Std\", model_df['ise_weights_std'].mean(), color=Fore.CYAN)\n",
    "        \n",
    "        print_result(\"KDE P Factor\", model_df['kde_p_factor'].iloc[0], color=Fore.YELLOW)\n",
    "        print_result(\"KDE G Factor\", model_df['kde_g_factor'].iloc[0], color=Fore.YELLOW)\n",
    "        \n",
    "        print_header(f\"Risk Method Correlation Analysis for {key}\")\n",
    "        risk_corr = model_df[['mce_p', 'mce_g', 'ise', 'kmm', 'classifier']].corr()\n",
    "        print(\"Correlation Matrix:\")\n",
    "        print(risk_corr.round(3))\n",
    "        \n",
    "        print_header(f\"Top Models Analysis for {key}\")\n",
    "        \n",
    "        final_results[key] = {}\n",
    "        \n",
    "        for method in ['MCE_g', 'ISE', 'KMM', 'Classifier']:\n",
    "            estimated_risks = np.array(all_risks[method])\n",
    "            true_risks = np.array(all_risks['MCE_p'])\n",
    "            \n",
    "            top_k_indices = np.argsort(estimated_risks)[:K]\n",
    "            selected_models = model_df.iloc[top_k_indices]\n",
    "            \n",
    "            r_final_method = np.mean(true_risks[top_k_indices])\n",
    "            final_results[key][method] = r_final_method\n",
    "            \n",
    "            print(f\"\\n{Fore.GREEN}Method: {method}{Style.RESET_ALL}\")\n",
    "            print_result(f\"R_final_{method}\", r_final_method, color=Fore.MAGENTA)\n",
    "            print_result(\"Selected Model AUCs G\", f\"{selected_models['auc_g'].mean():.3f} ± {selected_models['auc_g'].std():.3f}\")\n",
    "            print_result(\"Selected Model AUCs P\", f\"{selected_models['auc_p'].mean():.3f} ± {selected_models['auc_p'].std():.3f}\")\n",
    "            print_result(\"Selected Models Types\", f\"GB: {sum(selected_models['model_type'] == 'GradientBoostingClassifier')}, RF: {sum(selected_models['model_type'] == 'RandomForestClassifier')}\")\n",
    "        \n",
    "        baseline_top_k = np.argsort(all_risks['MCE_p'])[:K]\n",
    "        oracle_risk = np.mean(np.array(all_risks['MCE_p'])[baseline_top_k])\n",
    "        oracle_models = model_df.iloc[baseline_top_k]\n",
    "        final_results[key]['Oracle'] = oracle_risk\n",
    "        \n",
    "        print(f\"\\n{Fore.GREEN}Method: Oracle{Style.RESET_ALL}\")\n",
    "        print_result(\"R_final_Oracle\", oracle_risk, color=Fore.GREEN)\n",
    "        print_result(\"Oracle Model AUCs G\", f\"{oracle_models['auc_g'].mean():.3f} ± {oracle_models['auc_g'].std():.3f}\")\n",
    "        print_result(\"Oracle Model AUCs P\", f\"{oracle_models['auc_p'].mean():.3f} ± {oracle_models['auc_p'].std():.3f}\")\n",
    "        print_result(\"Oracle Models Types\", f\"GB: {sum(oracle_models['model_type'] == 'GradientBoostingClassifier')}, RF: {sum(oracle_models['model_type'] == 'RandomForestClassifier')}\")\n",
    "        \n",
    "        final_results[key]['model_stats'] = {\n",
    "            'total_models': len(models),\n",
    "            'auc_g_range': [model_df['auc_g'].min(), model_df['auc_g'].max()],\n",
    "            'auc_p_range': [model_df['auc_p'].min(), model_df['auc_p'].max()],\n",
    "            'risk_correlations': risk_corr['mce_p'].to_dict()\n",
    "        }\n",
    "\n",
    "print_header(\"FINAL RESULTS SUMMARY\")\n",
    "for dataset, results in final_results.items():\n",
    "    print(f\"\\n{Fore.YELLOW}{Style.BRIGHT}Dataset: {dataset}{Style.RESET_ALL}\")\n",
    "    for method, risk in results.items():\n",
    "        color = Fore.GREEN if method == 'Oracle' else Fore.WHITE\n",
    "        print_result(f\"  {method}\", risk, color=color)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
